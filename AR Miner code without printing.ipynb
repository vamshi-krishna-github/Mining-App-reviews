{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_file=\"C:\\\\Users\\\\vamshi krishna\\\\Desktop\\\\info.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenfive ratingone Can't change profile picture.\n",
      "lentwentyfive ratingfour Doesnt seem to be working everytime i go on it, it says it cant update anything so stuck with statuses from november saved on there!\n",
      "lenfour ratingone Take forever to upload ....\n",
      "leneight ratingone Painfully slow when it does decide to work.\n",
      "leneleven ratingone I continually experience freezing, black screens, my phone gets locked up...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f=open(info_file)\n",
    "l = f.read().splitlines()\n",
    "print(l[0] + \"\\n\" + l[1] + \"\\n\" + l[2] + \"\\n\" + l[3] + \"\\n\" + l[4] + \"\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets us do preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will extract unique words from \"info_file\" and add it to the dictonary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def add_words_to_dict(file_name):\n",
    "    stop_words = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\",\n",
    "                  \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\",\n",
    "                  \"having\", \"he\", \"hed\", \"hell\", \"hes\", \"her\", \"here\", \"heres\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"hows\", \"i\", \"id\", \"ill\", \"im\", \"ive\",\n",
    "                  \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"its\", \"itself\", \"lets\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\",\n",
    "                  \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"shed\", \"shell\", \"shes\", \"should\", \"so\",\n",
    "                  \"some\", \"such\", \"than\", \"that\", \"thats\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"theres\", \"these\", \"they\", \"theyd\", \"theyll\", \"theyre\",\n",
    "                  \"theyve\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"wed\", \"well\", \"were\", \"weve\", \"were\", \"what\", \"whats\", \"when\",\n",
    "                  \"whens\", \"where\", \"wheres\", \"which\", \"while\", \"who\", \"whos\", \"whom\", \"why\", \"whys\", \"with\", \"would\", \"you\", \"youd\", \"youll\", \"youre\", \"youve\",\n",
    "                  \"your\", \"yours\", \"yourself\", \"yourselves\", \"need\", \"needed\", \"can\", \"u\", \"every\", \"rather\", \"gonna\", \"m\", \"tap\", \"fish\", \"gives\", \"nice\", \"hoping\"]\n",
    "    word_id={}\n",
    "    \n",
    "    with open(file_name) as f:\n",
    "        lines=f.read().splitlines()\n",
    "    for line in lines:\n",
    "        line=re.sub(r'[^a-zA-Z\\s]', \"\", line)\n",
    "        words=line.split()\n",
    "        words=words[2:]\n",
    "        for word in words:\n",
    "            word=word.lower()\n",
    "            if word not in stop_words:\n",
    "                word_id[word]=0\n",
    "                \n",
    "    id=0\n",
    "    for word in word_id:\n",
    "        word_id[word]=id\n",
    "        id+=1\n",
    "        \n",
    "    return word_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictonary=add_words_to_dict(info_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cant - 0\n",
      "change - 1\n",
      "profile - 2\n",
      "picture - 3\n",
      "doesnt - 4\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for key,value in dictonary.items():\n",
    "    print(key + \" - \" + str(value))\n",
    "    c+=1\n",
    "    if c==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in info_file = 974\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique words in info_file = \" + str(len(dictonary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make feautue matrix where first 6 columns represents rating of the app and next 974 columns are the unique words in the info_file, so, a total of 980 features are formed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format of each app review is as follows: \"lenfour ratingone Notifications are falling apart.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_matrix(file_name, word_id):\n",
    "    rating_bits=6\n",
    "    cols=rating_bits+len(word_id) \n",
    "    data_list=[]\n",
    "    with open(file_name) as f:\n",
    "        lines=f.read().splitlines() #Split lines in document.\n",
    "    for line in lines:\n",
    "        line=line.lower()\n",
    "        rating=0\n",
    "        try:\n",
    "            rating_str=line.split(\" \", 2)[1] #extract rating by the string two times at white space.\n",
    "        except IndexError:\n",
    "            continue\n",
    "        if rating_str.endswith(\"one\"):\n",
    "            rating=1\n",
    "        elif rating_str.endswith(\"two\"):\n",
    "            rating=2\n",
    "        elif rating_str.endswith(\"three\"):\n",
    "            rating=3\n",
    "        elif rating_str.endswith(\"four\"):\n",
    "            rating=4\n",
    "        elif rating_str.endswith(\"five\"):\n",
    "            rating=5\n",
    "        \n",
    "        line=line.split(\" \", 2)[2] #Read the remaining review excluding length of review and rating.\n",
    "        line=re.sub(r'[\\.\\?]', \",\", line)\n",
    "        reviews=line.split(\",\")\n",
    "        for review in reviews:\n",
    "            if review==\"\":\n",
    "                continue\n",
    "            if re.match(r'[^a-zA-Z]', review) is not None:\n",
    "                continue\n",
    "            instance=np.zeros((cols, ), dtype=int) #initialize all columns to zero.\n",
    "            instance[rating]=1\n",
    "            words=review.split()\n",
    "            for word in words:\n",
    "                attr_idx=word_id.get(word, None)\n",
    "                if attr_idx is not None:\n",
    "                    instance[rating_bits+attr_idx]=1\n",
    "                if np.count_nonzero(instance[rating_bits:])!=0: #if all columns are not zero, then append\n",
    "                    data_list.append(instance)\n",
    "    \n",
    "    ret_val=np.array(data_list)\n",
    "    return ret_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_info_file=\"C:\\\\Users\\\\vamshi krishna\\\\Desktop\\\\non-info.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_info = get_feature_matrix(info_file, dictonary)\n",
    "training_data_non_info = get_feature_matrix(non_info_file, dictonary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_y_info_label = np.ones(training_data_info.shape[0], dtype=int)\n",
    "training_y_label = np.append(training_y_info_label, np.zeros(training_data_non_info.shape[0], dtype=int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.append(training_data_info, training_data_non_info, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we got training data with X-feature matrix (training_data) and y-matrix (training_y_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets us import test data set and will repeat the same process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_info=\"C:\\\\Users\\\\vamshi krishna\\\\Desktop\\\\test\\\\info.txt\"\n",
    "test_non_info=\"C:\\\\Users\\\\vamshi krishna\\\\Desktop\\\\test\\\\non-info.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x0 = get_feature_matrix(test_info, dictonary)\n",
    "test_y = np.ones(test_x0.shape[0], dtype=int)\n",
    "test_x1 = get_feature_matrix(test_non_info, dictonary)\n",
    "test_y = np.append(test_y, np.zeros(test_x1.shape[0], dtype=int))\n",
    "test_x = np.append(test_x0, test_x1, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform the powerful machine learning algorithm named Naive Bayes :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf=BernoulliNB()\n",
    "clf.fit(training_data, training_y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8238218333664744"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=clf.predict(test_x)\n",
    "result=np.array(result, dtype=bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try our model on unlabeled dataset and we divide that dataset into informative and non_informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data=\"C:\\\\Users\\\\vamshi krishna\\\\Desktop\\\\trainU\\\\New Text Document.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_matrix = get_feature_matrix(unlabeled_data, dictonary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = clf.predict(unlabeled_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = np.array(final_result, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones_count = np.count_nonzero(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19815,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent = ones_count/19815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6283118849356548"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precentage of informative reviews in unlabeled dataset is : 62.831188493565485\n"
     ]
    }
   ],
   "source": [
    "print(\"Precentage of informative reviews in unlabeled dataset is : \" + str(percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets put these these informative reviews in new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"C:\\\\Users\\\\vamshi krishna\\\\Desktop\\\\trainU\\\\New Text Document.txt\", 'r+')\n",
    "Lines = f.readlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file=open(\"C:\\\\Users\\\\vamshi krishna\\\\Desktop\\\\infor_file_new.txt\", 'a+') # infor_file_new is an empty text document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0;\n",
    "for line in Lines:\n",
    "    if(final_result[i]==1):\n",
    "        new_file.write(line)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The file named \"infor_file_new\" contains informative reviews of the unlabelled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That completes mining the app reviews"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
